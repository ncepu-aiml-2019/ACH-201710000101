{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class '_csv.reader'>\n",
      "1713\n",
      "1713\n",
      "<class '_csv.reader'>\n",
      "105\n",
      "['how', 'did', 'you', 'know', 'that', 'people', 'makes', 'another', 'account', 'just', 'for', 'subscribing', 'itself', 'and', 'liking']\n",
      "['gonna', 'share', 'little', 'ryhme', 'canibus', 'blows', 'eminem', 'away', 'quadrillion', 'times', 'especially', 'about', 'the', 'categories', 'intelligent', 'things', 'his', 'mind', 'that', 'learned', 'and', 'rapped', 'about', 'and', 'forgot', 'before', 'eminem', 'spit', 'his', 'first', 'ryme', 'luv', 'linz']\n",
      "['753', '682', '421', 'gangnam', 'style']\n",
      "['some', 'classsic']\n",
      "['hey', 'guys', 'love', 'this', 'but', 'check', 'out', 'this', 'girl', 'name', 'cause', 'she', 'knows', 'how', 'dance', 'search', 'dancing']\n",
      "['part', 'holy', 'mary', 'pray', 'for', 'holy', 'mother', 'god', 'pray', 'for', 'holy', 'virgin', 'virgins', 'pray', 'for', 'mother', 'christ', 'pray', 'for', 'mother', 'divine', 'grace', 'pray', 'for', 'mother', 'most', 'pure', 'pray', 'for', 'mother', 'most', 'chaste', 'pray', 'for', 'mother', 'inviolate', 'pray', 'for', 'mother', 'undefiled', 'pray', 'for', 'mother', 'most', 'amiable', 'pray', 'for', 'mother', 'most', 'admirable', 'pray', 'for', 'mother', 'good', 'counsel', 'pray', 'for', 'mother', 'our', 'creator', 'pray', 'for', 'mother', 'our', 'redeemer', 'pray', 'for']\n",
      "the error rate is 0.057692307692307696\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import csv\n",
    "import re\n",
    "def set_of_words2vec(vocab_list, input_set):\n",
    "    \"\"\"\n",
    "    遍历查看该单词是否出现，出现该单词则将该单词置1\n",
    "    :param vocab_list: 所有单词集合列表\n",
    "    :param input_set: 输入数据集\n",
    "    :return: 匹配列表[0,1,0,1...]，其中 1与0 表示词汇表中的单词是否出现在输入的数据集中\n",
    "    \"\"\"\n",
    "    # 创建一个和词汇表等长的向量，并将其元素都设置为0\n",
    "    result = [0] * len(vocab_list)\n",
    "    # 遍历文档中的所有单词，如果出现了词汇表中的单词，则将输出的文档向量中的对应值设为1\n",
    "    for word in input_set:\n",
    "        if word in vocab_list:\n",
    "            result[vocab_list.index(word)] = 1\n",
    "        else:\n",
    "            # 这个后面应该注释掉，因为对你没什么用，这只是为了辅助调试的\n",
    "            # print('the word: {} is not in my vocabulary'.format(word))\n",
    "            pass\n",
    "    return result\n",
    "\n",
    "\n",
    "def _train_naive_bayes(train_mat, train_category):\n",
    "    \"\"\"\n",
    "    朴素贝叶斯分类原版\n",
    "    :param train_mat:  type is ndarray\n",
    "                    总的输入文本，大致是 [[0,1,0,1], [], []]\n",
    "    :param train_category: 文件对应的类别分类， [0, 1, 0],\n",
    "                            列表的长度应该等于上面那个输入文本的长度\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    train_doc_num = len(train_mat)\n",
    "    words_num = len(train_mat[0])\n",
    "    # 因为侮辱性的被标记为了1， 所以只要把他们相加就可以得到侮辱性的有多少\n",
    "    # 侮辱性文件的出现概率，即train_category中所有的1的个数，\n",
    "    # 代表的就是多少个侮辱性文件，与文件的总数相除就得到了侮辱性文件的出现概率\n",
    "    pos_abusive = np.sum(train_category) / train_doc_num\n",
    "    # 单词出现的次数\n",
    "    # 原版\n",
    "    p0num = np.zeros(words_num)\n",
    "    p1num = np.zeros(words_num)\n",
    "\n",
    "    # 整个数据集单词出现的次数（原来是0，后面改成2了）\n",
    "    p0num_all = 0\n",
    "    p1num_all = 0\n",
    "\n",
    "    for i in range(train_doc_num):\n",
    "        # 遍历所有的文件，如果是侮辱性文件，就计算此侮辱性文件中出现的侮辱性单词的个数\n",
    "        if train_category[i] == 1:\n",
    "            p1num += train_mat[i]\n",
    "            p1num_all += np.sum(train_mat[i])\n",
    "        else:\n",
    "            p0num += train_mat[i]\n",
    "            p0num_all += np.sum(train_mat[i])\n",
    "    # 后面需要改成改成取 log 函数\n",
    "    p1vec = p1num / p1num_all\n",
    "    p0vec = p0num / p0num_all\n",
    "    return p0vec, p1vec, pos_abusive\n",
    "\n",
    "\n",
    "def train_naive_bayes(train_mat, train_category):\n",
    "    \"\"\"\n",
    "    朴素贝叶斯分类修正版，　注意和原来的对比，为什么这么做可以查看书\n",
    "    :param train_mat:  type is ndarray\n",
    "                    总的输入文本，大致是 [[0,1,0,1], [], []]\n",
    "    :param train_category: 文件对应的类别分类， [0, 1, 0],\n",
    "                            列表的长度应该等于上面那个输入文本的长度\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    train_doc_num = len(train_mat)\n",
    "    words_num = len(train_mat[0])\n",
    "    # 因为侮辱性的被标记为了1， 所以只要把他们相加就可以得到侮辱性的有多少\n",
    "    # 侮辱性文件的出现概率，即train_category中所有的1的个数，\n",
    "    # 代表的就是多少个侮辱性文件，与文件的总数相除就得到了侮辱性文件的出现概率\n",
    "    pos_abusive = np.sum(train_category) / train_doc_num\n",
    "    # 单词出现的次数\n",
    "    # 原版，变成ones是修改版，这是为了防止数字过小溢出\n",
    "    # p0num = np.zeros(words_num)\n",
    "    # p1num = np.zeros(words_num)\n",
    "    p0num = np.ones(words_num)\n",
    "    p1num = np.ones(words_num)\n",
    "    # 整个数据集单词出现的次数（原来是0，后面改成2了）\n",
    "    p0num_all = 2.0\n",
    "    p1num_all = 2.0\n",
    "\n",
    "    for i in range(train_doc_num):\n",
    "        # 遍历所有的文件，如果是侮辱性文件，就计算此侮辱性文件中出现的侮辱性单词的个数\n",
    "        if train_category[i] == 1:\n",
    "            p1num += train_mat[i]\n",
    "            p1num_all += np.sum(train_mat[i])\n",
    "        else:\n",
    "            p0num += train_mat[i]\n",
    "            p0num_all += np.sum(train_mat[i])\n",
    "    # 后面改成取 log 函数\n",
    "    p1vec = np.log(p1num / p1num_all)\n",
    "    p0vec = np.log(p0num / p0num_all)\n",
    "    return p0vec, p1vec, pos_abusive\n",
    "\n",
    "\n",
    "def classify_naive_bayes(vec2classify, p0vec, p1vec, p_class1):\n",
    "    \"\"\"\n",
    "    使用算法：\n",
    "        # 将乘法转换为加法\n",
    "        乘法：P(C|F1F2...Fn) = P(F1F2...Fn|C)P(C)/P(F1F2...Fn)\n",
    "        加法：P(F1|C)*P(F2|C)....P(Fn|C)P(C) -> log(P(F1|C))+log(P(F2|C))+....+log(P(Fn|C))+log(P(C))\n",
    "    :param vec2classify: 待测数据[0,1,1,1,1...]，即要分类的向量\n",
    "    :param p0vec: 类别0，即正常文档的[log(P(F1|C0)),log(P(F2|C0)),log(P(F3|C0)),log(P(F4|C0)),log(P(F5|C0))....]列表\n",
    "    :param p1vec: 类别1，即侮辱性文档的[log(P(F1|C1)),log(P(F2|C1)),log(P(F3|C1)),log(P(F4|C1)),log(P(F5|C1))....]列表\n",
    "    :param p_class1: 类别1，侮辱性文件的出现概率\n",
    "    :return: 类别1 or 0\n",
    "    \"\"\"\n",
    "    # 计算公式  log(P(F1|C))+log(P(F2|C))+....+log(P(Fn|C))+log(P(C))\n",
    "    # 使用 NumPy 数组来计算两个向量相乘的结果，这里的相乘是指对应元素相乘，即先将两个向量中的第一个元素相乘，然后将第2个元素相乘，以此类推。\n",
    "    # 我的理解是：这里的 vec2Classify * p1Vec 的意思就是将每个词与其对应的概率相关联起来\n",
    "    # 可以理解为 1.单词在词汇表中的条件下，文件是good 类别的概率 也可以理解为 2.在整个空间下，文件既在词汇表中又是good类别的概率\n",
    "    p1 = np.sum(vec2classify * p1vec) + np.log(p_class1)\n",
    "    p0 = np.sum(vec2classify * p0vec) + np.log(1 - p_class1)\n",
    "    if p1 > p0:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "def create_vocab_list(data_set):\n",
    "    \"\"\"\n",
    "    获取所有单词的集合\n",
    "    :param data_set: 数据集\n",
    "    :return: 所有单词的集合(即不含重复元素的单词列表)\n",
    "    \"\"\"\n",
    "    vocab_set = set()  # create empty set\n",
    "    for item in data_set:\n",
    "        # | 求两个集合的并集\n",
    "        vocab_set = vocab_set | set(item)\n",
    "    return list(vocab_set)\n",
    "# 接收一个大写字符串并将其解析成字符串列表\n",
    "def text_parse(big_str):\n",
    "    \"\"\"\n",
    "    这里就是做词划分\n",
    "    :param big_str: 某个被拼接后的字符串\n",
    "    :return: 全部是小写的word列表，去掉少于 2 个字符的字符串\n",
    "    \"\"\"\n",
    "    import re\n",
    "    # 其实这里比较推荐用　\\W+ 代替 \\W*，\n",
    "    # 因为 \\W*会match empty patten，在py3.5+之后就会出现什么问题，推荐自己修改尝试一下，可能就会re.split理解更深了\n",
    "    token_list = re.split(r'\\W+', big_str)\n",
    "    if len(token_list) == 0:\n",
    "        print(token_list)\n",
    "    return [tok.lower() for tok in token_list if len(tok) > 2]\n",
    "\n",
    "# 导入文件夹中数据，并将它们解析成词列表\n",
    "def spam_test():\n",
    "    \"\"\"\n",
    "    对贝叶斯垃圾邮件分类器进行自动化处理。\n",
    "    :return: nothing\n",
    "    \"\"\"\n",
    "    doc_list = []\n",
    "    class_list = []# 标识列表，这里是标识侮辱和非侮辱的词语\n",
    "    full_text = []\n",
    "    #添加文档\n",
    "    doc_list,full_text,class_list = openFile(r'C:\\Users\\admin\\Desktop\\machine-learning\\ai\\datasets\\YoutubeCommentsSpam\\YoutubeCommentsSpam_train.csv')\n",
    "    # 创建词汇表\n",
    "    # print(1)\n",
    "    vocab_list = create_vocab_list(doc_list)#创建一个词汇表\n",
    "\n",
    "    # import random\n",
    "    # 生成随机取10个数, 为了避免警告将每个数都转换为整型\n",
    "    # test_set = [int(num) for num in random.sample(range(1853), 300)]\n",
    "    # 并在原来的training_set中去掉这10个数\n",
    "    print(len(class_list))\n",
    "    training_set = list(set(range(len(class_list)-1)))\n",
    "    training_mat = []\n",
    "    training_class = []\n",
    "    print(len(doc_list))\n",
    "    for doc_index in training_set:\n",
    "        # print(doc_index)\n",
    "        training_mat.append(set_of_words2vec(vocab_list, doc_list[doc_index]))#转化为词向量\n",
    "        training_class.append(class_list[doc_index])# 将0,1等提出来\n",
    "    p0v, p1v, p_spam = train_naive_bayes(\n",
    "        np.array(training_mat),\n",
    "        np.array(training_class)\n",
    "    )#通过训练集的到预测函数\n",
    "\n",
    "    error_count = 0\n",
    "    i = 1\n",
    "    doc_list1 = []\n",
    "    full_text1 = []\n",
    "    class_list1 = []\n",
    "    doc_list1,full_text1,class_list1 = openFile(r'C:\\Users\\admin\\Desktop\\machine-learning\\ai\\datasets\\YoutubeCommentsSpam\\YoutubeCommentsSpam_test.csv')\n",
    "    print(len(class_list1))\n",
    "    testset = list(range(len(class_list1)-1))\n",
    "    for doc_index in testset:\n",
    "\n",
    "        word_vec = set_of_words2vec(vocab_list, doc_list1[doc_index])\n",
    "        if classify_naive_bayes(\n",
    "                np.array(word_vec),\n",
    "                p0v,\n",
    "                p1v,\n",
    "                p_spam\n",
    "        ) != class_list1[doc_index]:#这个就是预测结果\n",
    "            print(doc_list1[doc_index])\n",
    "            error_count += 1\n",
    "        # print(word_vec)\n",
    "    print('the error rate is {}'.format(\n",
    "        error_count / len(testset)\n",
    "    ))\n",
    "def openFile(round:str):\n",
    "    doc_list = []\n",
    "    class_list = []# 标识列表，这里是标识侮辱和非侮辱的词语\n",
    "    full_text = []\n",
    "    csv_file = open(round)\n",
    "    csv_reader_lines = csv.reader(csv_file)  # 逐行读取csv文件\n",
    "    print(type(csv_reader_lines))\n",
    "    date = []  # 创建列表准备接收csv各行数据\n",
    "    for one_line in csv_reader_lines:\n",
    "        if(re.search(u\"[\\u4e00-\\u9fa5]+\",one_line[0])==None):#将中文乱码抹去\n",
    "            date.append(one_line)  # 将读取的csv分行数据按行存入列表‘date’中\n",
    "    i = 1\n",
    "    while i < len(date):\n",
    "        words = text_parse(date[i][0])\n",
    "        doc_list.append(words)\n",
    "        full_text.extend(words)\n",
    "        # print(date[i][1])\n",
    "        class_list.append(int(date[i][1]))\n",
    "        i+=1\n",
    "    return  doc_list,full_text,class_list\n",
    "spam_test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
